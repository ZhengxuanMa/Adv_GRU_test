{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-1ab430620609>:59: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-18-1ab430620609>:90: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-18-1ab430620609>:117: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/rice/notebook/.packages/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/rice/notebook/.packages/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell.py:104: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "##Stacked GRU - 2layers with attention mechanism \n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import tensorflow\n",
    "except ImportError:\n",
    "    # install qlib\n",
    "    !pip install gast==0.2.2\n",
    "    !pip install tensorflow==1.14.0\n",
    "    \n",
    "from tensorflow.contrib.rnn.python.ops.rnn_cell import AttentionCellWrapper\n",
    "from tensorflow.contrib.rnn.python.ops.gru_ops import GRUBlockCellV2\n",
    "from tensorflow.compat.v1.nn.rnn_cell import DropoutWrapper\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "import tensorflow as tf\n",
    "import talib \n",
    "import datetime\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import time\n",
    "import os\n",
    "pd.set_option('display.max_rows', 500)\n",
    "try:\n",
    "    os.chdir('Models')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(os.getcwd())\n",
    "batch_size = 100\n",
    "in_length = 48\n",
    "#504 æ˜¯ (240/30) * (feature 63) \n",
    "in_width = 504\n",
    "num_units = 128\n",
    "num_dims = 2\n",
    "outclasses = 3\n",
    "#learning_rate = 3e-4\n",
    "learning_rate = 0.001\n",
    "max_grad_norm = 5\n",
    "\n",
    "start = '2016-06-01'\n",
    "end = '2021-01-01'\n",
    "\n",
    "\n",
    "cells_f = []\n",
    "cells_b = []\n",
    "n_layers = 3\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(shape=[], dtype=tf.float32)\n",
    "keep_in_prob = tf.placeholder(shape=[], dtype=tf.float32)\n",
    "\n",
    "batch_sizee = tf.placeholder(tf.int64)\n",
    "MAX_COMP = 11\n",
    "\n",
    "attention_layer_fw1= GRUBlockCellV2(num_units=n_in)\n",
    "\n",
    "attention_layer_fw1 = AttentionCellWrapper(attention_layer_fw1,attn_length=2)\n",
    "\n",
    "attention_layer_fw1 = tf.contrib.rnn.DropoutWrapper(attention_layer_fw1,output_keep_prob=keep_prob,input_keep_prob=keep_in_prob)\n",
    "\n",
    "cells_f.append(attention_layer_fw1)\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "attention_layer_fw2= GRUBlockCellV2(num_units=n_in)\n",
    "\n",
    "attention_layer_fw2 = AttentionCellWrapper(attention_layer_fw2,attn_length=2)\n",
    "\n",
    "attention_layer_fw2 = tf.contrib.rnn.DropoutWrapper(attention_layer_fw2,output_keep_prob=keep_prob,input_keep_prob=keep_in_prob)\n",
    "\n",
    "cells_f.append(attention_layer_fw2)\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "cell_f = tf.contrib.rnn.MultiRNNCell(cells_f, state_is_tuple=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_data = tf.placeholder(shape=[None, in_length, 8,col_after_drop], dtype=tf.float32)\n",
    "\n",
    "X_input = tf.transpose(a=input_data, perm=[1,0,2,3])\n",
    "\n",
    "\n",
    "targets = tf.placeholder(shape=[None,outclasses], dtype=tf.float32)\n",
    "\n",
    "\n",
    "X_input_2  = tf.reshape(X_input,[in_length,-1,8*col_after_drop])\n",
    "\n",
    "\n",
    "X_input_3 = tf.layers.dense(X_input_2,units=in_width,activation=tf.nn.tanh, name='in_fc',\n",
    "    kernel_initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outputs,new_state = tf.nn.dynamic_rnn(cell=cell_f, \\\n",
    "                          inputs=X_input_3, \\\n",
    "                          initial_state=None, \\\n",
    "                          time_major=True,\\\n",
    "                             dtype=tf.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequences = outputs\n",
    "loss = 0\n",
    "adv_loss = 0\n",
    "l2_norm = 0\n",
    "\n",
    "adv_train= 1\n",
    "hinge = 1\n",
    "att=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('lstm2') as scope:\n",
    "    ##myloss:\n",
    "    \n",
    "    \n",
    "    outputs = tf.transpose(output_sequences,[1,0,2])\n",
    "    \n",
    "    \n",
    "    av_W = tf.get_variable(\n",
    "      name='att_W', dtype=tf.float32,\n",
    "      shape=[n_in*1,n_in*1],\n",
    "      initializer=tf.glorot_uniform_initializer()\n",
    "      )\n",
    "    av_b = tf.get_variable(\n",
    "          name='att_h', dtype=tf.float32,\n",
    "          shape=[n_in*1],\n",
    "          initializer=tf.zeros_initializer())\n",
    "        \n",
    "    av_u = tf.get_variable(\n",
    "          name='att_u', dtype=tf.float32,\n",
    "          shape=[n_in*1],\n",
    "          initializer=tf.glorot_uniform_initializer()\n",
    "      )\n",
    "\n",
    "    a_laten = tf.tanh(\n",
    "          tf.tensordot(outputs,av_W,\n",
    "                        axes=1) + av_b)\n",
    "    a_scores = tf.tensordot(a_laten, av_u,\n",
    "                                    axes=1,\n",
    "                                    name='scores')\n",
    "    a_alphas = tf.nn.softmax(a_scores, name='alphas')\n",
    "\n",
    "    a_con = tf.reduce_sum(\n",
    "          outputs * tf.expand_dims(a_alphas, -1), 1)\n",
    "    \n",
    "    fea_con = tf.concat(\n",
    "      [outputs[:, -1, :], a_con],\n",
    "      axis=1)\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('pre_fcc2'):\n",
    "    \n",
    "    \n",
    "    fc_W = tf.get_variable(\n",
    "        'weights', dtype=tf.float32,\n",
    "        shape=[n_in*1*2, outclasses],\n",
    "        initializer=tf.glorot_uniform_initializer()\n",
    "    )\n",
    "    fc_b = tf.get_variable(\n",
    "        'biases', dtype=tf.float32,\n",
    "        shape=[outclasses],\n",
    "        initializer=tf.zeros_initializer()\n",
    "    )\n",
    "\n",
    "########my algo is basically upto this part, pred of this code equals final_output of my code##\n",
    "   \n",
    "    final_output = tf.nn.bias_add(\n",
    "            tf.matmul(fea_con, fc_W), fc_b\n",
    "        )\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-23-32f2dc12f9d6>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=final_output, labels=targets))\n",
    "\n",
    "loss =cost\n",
    "\n",
    "\n",
    "##ADV Loss is implemented below: ##\n",
    "\n",
    "adv_loss = loss * 0\n",
    "#tf.compat.v1.reset_default_graph()\n",
    "#outputs = tf.reverse(tensor=outputs, axis=[0])[0,:,:]\n",
    "delta_adv= tf.gradients(loss, [fea_con])[0]\n",
    "#tf.stop_gradient(delta_adv)\n",
    "delta_adv = tf.stop_gradient(delta_adv)\n",
    "delta_adv = tf.nn.l2_normalize(delta_adv, axis=1)\n",
    "adv_pv_var = fea_con + \\\n",
    "                        0.6 * delta_adv\n",
    "scope.reuse_variables()\n",
    "\n",
    "with tf.variable_scope('pre_fcc2',reuse=True):\n",
    "    fc_W = tf.get_variable(\n",
    "        'weights', dtype=tf.float32,\n",
    "        shape=[n_in*1*2, outclasses],\n",
    "        initializer=tf.truncated_normal_initializer()\n",
    "\n",
    "    )\n",
    "    fc_b = tf.get_variable(\n",
    "        'biases', dtype=tf.float32,\n",
    "        shape=[outclasses],\n",
    "        initializer=tf.zeros_initializer()\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    adv_pred = tf.nn.bias_add(\n",
    "            tf.matmul(adv_pv_var, fc_W), fc_b\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "adv_loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=adv_pred, labels=targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correct_pred = tf.equal(tf.argmax(final_output,1), tf.argmax(targets,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "##Using the entire dataset - but this could be divided into training , validation and testing sets if required\n",
    "\n",
    "test_X = trainX\n",
    "test_Y = trainY\n",
    "\n",
    "\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "acc_pred_list = []\n",
    "adv_loss_list = []\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rice/notebook/.packages/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    }
   ],
   "source": [
    "#with tf.variable_scope('lstm2') as scope:\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "lr = tf.train.exponential_decay(\n",
    "                learning_rate = learning_rate,\n",
    "                decay_steps=10,\n",
    "                global_step = global_step,\n",
    "                decay_rate = 0.85,\n",
    "                staircase=True\n",
    "            )\n",
    "\n",
    "_trainable_variables = tf.trainable_variables()\n",
    "l2_coefficient = 97e-3\n",
    "_l2_regularizer = tf.add_n([tf.nn.l2_loss(v) for v in _trainable_variables])\n",
    "_l2_regularizer = _l2_regularizer * l2_coefficient / len(_trainable_variables)\n",
    "\n",
    "\n",
    "\n",
    "obj_func = cost +lr * adv_loss +_l2_regularizer \n",
    "\n",
    "# cost1 = tf.reduce_sum(tf.nn.weighted_cross_entropy_with_logits(logits=final_output, labels=targets,pos_weight=0.8))\n",
    "# cost = cost1+ _l2_regularizer\n",
    "\n",
    "params = tf.trainable_variables()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "gvs = optimizer.compute_gradients(obj_func, params)\n",
    "ysss = []\n",
    "for i, (grid,var) in enumerate(gvs):\n",
    "    if grid != None:\n",
    "        grid = tf.clip_by_value(grid,-5.,5.)\n",
    "        gvs[i] = (grid,var)\n",
    "optimizer = optimizer.apply_gradients(gvs,global_step = global_step)\n",
    "\n",
    "model_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "is_training = tf.get_variable('is_training', shape=(), dtype=tf.bool,\n",
    "                              initializer=tf.constant_initializer(True, dtype=tf.bool))\n",
    "\n",
    "\n",
    "ema = tf.train.ExponentialMovingAverage(decay=0.9988)\n",
    "\n",
    "with tf.control_dependencies([optimizer]):\n",
    "    \n",
    "    train_op = ema.apply(model_vars)\n",
    "    \n",
    "    \n",
    "# Make backup variables\n",
    "# Make backup variables\n",
    "# Make backup variables\n",
    "\n",
    "with tf.variable_scope('BackupVariables'):\n",
    "    backup_vars = [tf.get_variable(var.op.name, dtype=var.value().dtype, trainable=False,\n",
    "                                   initializer=var.initialized_value())\n",
    "                   for var in model_vars]\n",
    "    \n",
    "\n",
    "def ema_to_weights():\n",
    "    return tf.group(*(tf.assign(var, ema.average(var).read_value())\n",
    "                     for var in model_vars))\n",
    "def save_weight_backups():\n",
    "    return tf.group(*(tf.assign(bck, var.read_value())\n",
    "                     for var, bck in zip(model_vars, backup_vars)))\n",
    "def restore_weight_backups():\n",
    "    return tf.group(*(tf.assign(var, bck.read_value())\n",
    "                     for var, bck in zip(model_vars, backup_vars)))\n",
    "\n",
    "def to_training():\n",
    "    with tf.control_dependencies([tf.assign(is_training, True)]):\n",
    "        return restore_weight_backups()\n",
    "\n",
    "def to_testing():\n",
    "    with tf.control_dependencies([tf.assign(is_training, False)]):\n",
    "        with tf.control_dependencies([save_weight_backups()]):\n",
    "            return ema_to_weights()\n",
    "        \n",
    "    \n",
    "switch_to_train_mode_op = tf.cond(is_training, lambda: tf.group(), to_training)\n",
    "switch_to_test_mode_op = tf.cond(is_training, to_testing, lambda: tf.group())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "global_step: 1\n",
      "0.6942308 LOSS:   951.8249 ADV_LOSS:   1708.7864\n",
      "It has been 93.01626229286194 seconds since the loop started\n",
      "1\n",
      "global_step: 2\n",
      "It has been 70.72398352622986 seconds since the loop started\n",
      "2\n",
      "global_step: 3\n",
      "It has been 72.17823910713196 seconds since the loop started\n",
      "3\n",
      "global_step: 4\n",
      "It has been 72.42990779876709 seconds since the loop started\n",
      "4\n",
      "global_step: 5\n",
      "It has been 72.55018496513367 seconds since the loop started\n",
      "5\n",
      "global_step: 6\n",
      "0.69711536 LOSS:   901.57385 ADV_LOSS:   1636.4192\n",
      "It has been 92.03213858604431 seconds since the loop started\n",
      "6\n",
      "global_step: 7\n",
      "It has been 74.30415511131287 seconds since the loop started\n",
      "7\n",
      "global_step: 8\n",
      "It has been 74.77837228775024 seconds since the loop started\n",
      "8\n",
      "global_step: 9\n",
      "It has been 74.66608810424805 seconds since the loop started\n",
      "9\n",
      "global_step: 10\n",
      "It has been 73.97279453277588 seconds since the loop started\n",
      "10\n",
      "global_step: 11\n",
      "0.70096153 LOSS:   811.63184 ADV_LOSS:   1473.1483\n",
      "It has been 92.99013566970825 seconds since the loop started\n",
      "11\n",
      "global_step: 12\n",
      "It has been 73.28534984588623 seconds since the loop started\n",
      "12\n",
      "global_step: 13\n",
      "It has been 72.26298975944519 seconds since the loop started\n",
      "13\n",
      "global_step: 14\n",
      "It has been 71.97939395904541 seconds since the loop started\n",
      "14\n",
      "global_step: 15\n",
      "It has been 72.4615490436554 seconds since the loop started\n",
      "15\n",
      "global_step: 16\n",
      "0.7153846 LOSS:   748.50476 ADV_LOSS:   1325.1399\n",
      "It has been 90.79409170150757 seconds since the loop started\n",
      "16\n",
      "global_step: 17\n",
      "It has been 73.70703172683716 seconds since the loop started\n",
      "17\n",
      "global_step: 18\n",
      "It has been 71.89603924751282 seconds since the loop started\n",
      "18\n",
      "global_step: 19\n",
      "It has been 73.78375148773193 seconds since the loop started\n",
      "19\n",
      "global_step: 20\n",
      "It has been 72.81448650360107 seconds since the loop started\n",
      "20\n",
      "global_step: 21\n",
      "0.7413462 LOSS:   674.84985 ADV_LOSS:   1192.6187\n",
      "It has been 91.775963306427 seconds since the loop started\n",
      "21\n",
      "global_step: 22\n",
      "It has been 72.84322476387024 seconds since the loop started\n",
      "22\n",
      "global_step: 23\n",
      "It has been 72.37858867645264 seconds since the loop started\n",
      "23\n",
      "global_step: 24\n",
      "It has been 72.77791285514832 seconds since the loop started\n",
      "24\n",
      "global_step: 25\n",
      "It has been 71.8638710975647 seconds since the loop started\n",
      "25\n",
      "global_step: 26\n",
      "0.7846154 LOSS:   584.8595 ADV_LOSS:   1051.7188\n",
      "It has been 93.06882691383362 seconds since the loop started\n",
      "26\n",
      "global_step: 27\n",
      "It has been 72.8867244720459 seconds since the loop started\n",
      "27\n",
      "global_step: 28\n",
      "It has been 73.29572224617004 seconds since the loop started\n",
      "28\n",
      "global_step: 29\n",
      "It has been 72.88752245903015 seconds since the loop started\n",
      "29\n",
      "global_step: 30\n",
      "It has been 72.69584798812866 seconds since the loop started\n",
      "30\n",
      "global_step: 31\n",
      "0.82019234 LOSS:   498.96906 ADV_LOSS:   946.5439\n",
      "It has been 91.69367980957031 seconds since the loop started\n",
      "31\n",
      "global_step: 32\n",
      "It has been 72.16523838043213 seconds since the loop started\n",
      "32\n",
      "global_step: 33\n",
      "It has been 72.15993618965149 seconds since the loop started\n",
      "33\n",
      "global_step: 34\n",
      "It has been 72.28542876243591 seconds since the loop started\n",
      "34\n",
      "global_step: 35\n",
      "It has been 72.3658618927002 seconds since the loop started\n",
      "35\n",
      "global_step: 36\n",
      "0.85192305 LOSS:   429.77655 ADV_LOSS:   834.7145\n",
      "It has been 92.27733707427979 seconds since the loop started\n",
      "36\n",
      "global_step: 37\n",
      "It has been 72.28213906288147 seconds since the loop started\n",
      "37\n",
      "global_step: 38\n",
      "It has been 72.96019101142883 seconds since the loop started\n",
      "38\n",
      "global_step: 39\n",
      "It has been 71.4582986831665 seconds since the loop started\n",
      "39\n",
      "global_step: 40\n",
      "It has been 72.77627682685852 seconds since the loop started\n",
      "40\n",
      "global_step: 41\n",
      "0.85 LOSS:   391.02658 ADV_LOSS:   770.7008\n",
      "It has been 91.74314713478088 seconds since the loop started\n",
      "41\n",
      "global_step: 42\n",
      "It has been 72.68250274658203 seconds since the loop started\n",
      "42\n",
      "global_step: 43\n",
      "It has been 72.75655770301819 seconds since the loop started\n",
      "43\n",
      "global_step: 44\n",
      "It has been 72.68195009231567 seconds since the loop started\n",
      "44\n",
      "global_step: 45\n",
      "It has been 72.4630753993988 seconds since the loop started\n",
      "45\n",
      "global_step: 46\n",
      "0.8528846 LOSS:   348.2531 ADV_LOSS:   698.6494\n",
      "It has been 90.86079931259155 seconds since the loop started\n",
      "46\n",
      "global_step: 47\n",
      "It has been 73.4889976978302 seconds since the loop started\n",
      "47\n",
      "global_step: 48\n",
      "It has been 74.09866094589233 seconds since the loop started\n",
      "48\n",
      "global_step: 49\n",
      "It has been 73.31234288215637 seconds since the loop started\n",
      "49\n",
      "global_step: 50\n",
      "It has been 72.65089821815491 seconds since the loop started\n",
      "50\n",
      "global_step: 51\n",
      "0.8769231 LOSS:   314.68103 ADV_LOSS:   634.2356\n",
      "It has been 91.35628271102905 seconds since the loop started\n",
      "51\n",
      "global_step: 52\n",
      "It has been 73.16956186294556 seconds since the loop started\n",
      "52\n",
      "global_step: 53\n",
      "It has been 74.1696400642395 seconds since the loop started\n",
      "53\n",
      "global_step: 54\n",
      "It has been 71.8070125579834 seconds since the loop started\n",
      "54\n",
      "global_step: 55\n",
      "It has been 72.66660952568054 seconds since the loop started\n",
      "55\n",
      "global_step: 56\n",
      "0.8894231 LOSS:   292.14368 ADV_LOSS:   598.9543\n",
      "It has been 91.37766170501709 seconds since the loop started\n",
      "56\n",
      "global_step: 57\n",
      "It has been 73.63219785690308 seconds since the loop started\n",
      "57\n",
      "global_step: 58\n",
      "It has been 72.86857724189758 seconds since the loop started\n",
      "58\n",
      "global_step: 59\n",
      "It has been 73.6825783252716 seconds since the loop started\n",
      "59\n",
      "global_step: 60\n",
      "It has been 73.44213604927063 seconds since the loop started\n",
      "60\n",
      "global_step: 61\n",
      "0.88269234 LOSS:   285.37915 ADV_LOSS:   575.761\n",
      "It has been 93.4799017906189 seconds since the loop started\n",
      "61\n",
      "global_step: 62\n",
      "It has been 74.09529495239258 seconds since the loop started\n",
      "62\n",
      "global_step: 63\n",
      "It has been 72.94876909255981 seconds since the loop started\n",
      "63\n",
      "global_step: 64\n",
      "It has been 73.39169812202454 seconds since the loop started\n",
      "64\n",
      "global_step: 65\n",
      "It has been 73.3604507446289 seconds since the loop started\n",
      "65\n",
      "global_step: 66\n",
      "0.89326924 LOSS:   254.60092 ADV_LOSS:   534.2794\n",
      "It has been 93.4609887599945 seconds since the loop started\n",
      "66\n",
      "global_step: 67\n",
      "It has been 74.67655086517334 seconds since the loop started\n",
      "67\n",
      "global_step: 68\n",
      "It has been 74.77731347084045 seconds since the loop started\n",
      "68\n",
      "global_step: 69\n",
      "It has been 74.27032017707825 seconds since the loop started\n",
      "69\n",
      "global_step: 70\n",
      "It has been 73.6555528640747 seconds since the loop started\n",
      "70\n",
      "global_step: 71\n",
      "0.9057692 LOSS:   245.67215 ADV_LOSS:   519.98663\n",
      "It has been 94.85092258453369 seconds since the loop started\n",
      "71\n",
      "global_step: 72\n",
      "It has been 74.22632765769958 seconds since the loop started\n",
      "72\n",
      "global_step: 73\n",
      "It has been 74.98963618278503 seconds since the loop started\n",
      "73\n",
      "global_step: 74\n",
      "It has been 74.88082790374756 seconds since the loop started\n",
      "74\n",
      "global_step: 75\n",
      "It has been 74.37606573104858 seconds since the loop started\n",
      "75\n",
      "global_step: 76\n",
      "0.9057692 LOSS:   228.88403 ADV_LOSS:   500.646\n",
      "It has been 94.25601363182068 seconds since the loop started\n",
      "76\n",
      "global_step: 77\n",
      "It has been 75.65735745429993 seconds since the loop started\n",
      "77\n",
      "global_step: 78\n",
      "It has been 75.87402558326721 seconds since the loop started\n",
      "78\n",
      "global_step: 79\n",
      "It has been 74.87063646316528 seconds since the loop started\n",
      "79\n",
      "global_step: 80\n",
      "It has been 74.9766173362732 seconds since the loop started\n",
      "80\n",
      "global_step: 81\n",
      "0.89807695 LOSS:   231.5567 ADV_LOSS:   504.21332\n",
      "It has been 96.39202618598938 seconds since the loop started\n",
      "81\n",
      "global_step: 82\n",
      "It has been 74.85792207717896 seconds since the loop started\n",
      "82\n",
      "global_step: 83\n",
      "It has been 74.25764894485474 seconds since the loop started\n",
      "83\n",
      "global_step: 84\n",
      "It has been 74.96682333946228 seconds since the loop started\n",
      "84\n",
      "global_step: 85\n",
      "It has been 75.47146129608154 seconds since the loop started\n",
      "85\n",
      "global_step: 86\n",
      "0.9057692 LOSS:   223.90463 ADV_LOSS:   484.8438\n",
      "It has been 97.08490419387817 seconds since the loop started\n",
      "86\n",
      "global_step: 87\n",
      "It has been 76.99980187416077 seconds since the loop started\n",
      "87\n",
      "global_step: 88\n",
      "It has been 77.15994215011597 seconds since the loop started\n",
      "88\n",
      "global_step: 89\n",
      "It has been 77.06548237800598 seconds since the loop started\n",
      "89\n",
      "global_step: 90\n",
      "It has been 78.08324861526489 seconds since the loop started\n",
      "90\n",
      "global_step: 91\n",
      "0.9269231 LOSS:   205.91776 ADV_LOSS:   458.27957\n",
      "It has been 97.7601113319397 seconds since the loop started\n",
      "91\n",
      "global_step: 92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It has been 78.27773976325989 seconds since the loop started\n",
      "92\n",
      "global_step: 93\n",
      "It has been 76.27825164794922 seconds since the loop started\n",
      "93\n",
      "global_step: 94\n",
      "It has been 78.89975333213806 seconds since the loop started\n",
      "94\n",
      "global_step: 95\n",
      "It has been 77.74976563453674 seconds since the loop started\n",
      "95\n",
      "global_step: 96\n",
      "0.9221154 LOSS:   200.04599 ADV_LOSS:   443.9931\n",
      "It has been 98.88061261177063 seconds since the loop started\n",
      "96\n",
      "global_step: 97\n",
      "It has been 75.36745858192444 seconds since the loop started\n",
      "97\n",
      "global_step: 98\n",
      "It has been 74.74681973457336 seconds since the loop started\n",
      "98\n",
      "global_step: 99\n",
      "It has been 77.86624503135681 seconds since the loop started\n",
      "99\n",
      "global_step: 100\n",
      "It has been 77.89281845092773 seconds since the loop started\n",
      "100\n",
      "global_step: 101\n",
      "0.91923076 LOSS:   198.29224 ADV_LOSS:   442.78696\n",
      "It has been 97.54989194869995 seconds since the loop started\n",
      "101\n",
      "global_step: 102\n",
      "It has been 77.45481896400452 seconds since the loop started\n",
      "102\n",
      "global_step: 103\n",
      "It has been 77.9726984500885 seconds since the loop started\n",
      "103\n",
      "global_step: 104\n",
      "It has been 78.46828842163086 seconds since the loop started\n",
      "104\n",
      "global_step: 105\n",
      "It has been 78.15740275382996 seconds since the loop started\n",
      "105\n",
      "global_step: 106\n",
      "0.93173075 LOSS:   196.79355 ADV_LOSS:   437.16913\n",
      "It has been 98.48997211456299 seconds since the loop started\n",
      "106\n",
      "global_step: 107\n",
      "It has been 78.0931088924408 seconds since the loop started\n",
      "107\n",
      "global_step: 108\n",
      "It has been 77.37336230278015 seconds since the loop started\n",
      "108\n",
      "global_step: 109\n",
      "It has been 78.14203834533691 seconds since the loop started\n",
      "109\n",
      "global_step: 110\n",
      "It has been 76.79205179214478 seconds since the loop started\n",
      "110\n",
      "global_step: 111\n",
      "0.92596155 LOSS:   183.89024 ADV_LOSS:   411.5846\n",
      "It has been 97.363853931427 seconds since the loop started\n",
      "111\n",
      "global_step: 112\n",
      "It has been 78.2573606967926 seconds since the loop started\n",
      "112\n",
      "global_step: 113\n",
      "It has been 79.9513692855835 seconds since the loop started\n",
      "113\n",
      "global_step: 114\n",
      "It has been 77.78163719177246 seconds since the loop started\n",
      "114\n",
      "global_step: 115\n",
      "It has been 78.58569526672363 seconds since the loop started\n",
      "115\n",
      "global_step: 116\n",
      "0.93365383 LOSS:   179.19917 ADV_LOSS:   403.76352\n",
      "It has been 99.07402014732361 seconds since the loop started\n",
      "116\n",
      "global_step: 117\n",
      "It has been 79.77823495864868 seconds since the loop started\n",
      "117\n",
      "global_step: 118\n",
      "It has been 79.81738209724426 seconds since the loop started\n",
      "118\n",
      "global_step: 119\n",
      "It has been 78.69925141334534 seconds since the loop started\n",
      "119\n",
      "global_step: 120\n",
      "It has been 78.88118720054626 seconds since the loop started\n",
      "120\n",
      "global_step: 121\n",
      "0.93173075 LOSS:   175.04129 ADV_LOSS:   391.80902\n",
      "It has been 98.89475154876709 seconds since the loop started\n",
      "121\n",
      "global_step: 122\n",
      "It has been 78.17599821090698 seconds since the loop started\n",
      "122\n",
      "global_step: 123\n",
      "It has been 79.44372940063477 seconds since the loop started\n",
      "123\n",
      "global_step: 124\n",
      "It has been 79.80374908447266 seconds since the loop started\n",
      "124\n",
      "global_step: 125\n",
      "It has been 79.60867023468018 seconds since the loop started\n",
      "125\n",
      "global_step: 126\n",
      "0.93653846 LOSS:   178.90265 ADV_LOSS:   398.9389\n",
      "It has been 100.64785361289978 seconds since the loop started\n",
      "126\n",
      "global_step: 127\n",
      "It has been 78.27607035636902 seconds since the loop started\n",
      "127\n",
      "global_step: 128\n",
      "It has been 79.9490487575531 seconds since the loop started\n",
      "128\n",
      "global_step: 129\n",
      "It has been 81.05348181724548 seconds since the loop started\n",
      "129\n",
      "global_step: 130\n",
      "It has been 79.78088140487671 seconds since the loop started\n",
      "130\n",
      "global_step: 131\n",
      "0.94134617 LOSS:   164.88126 ADV_LOSS:   377.7327\n",
      "It has been 100.18937063217163 seconds since the loop started\n",
      "131\n",
      "global_step: 132\n",
      "It has been 80.08984971046448 seconds since the loop started\n",
      "132\n",
      "global_step: 133\n",
      "It has been 79.64313673973083 seconds since the loop started\n",
      "133\n",
      "global_step: 134\n",
      "It has been 79.3973798751831 seconds since the loop started\n",
      "134\n",
      "global_step: 135\n",
      "It has been 80.15135192871094 seconds since the loop started\n",
      "135\n",
      "global_step: 136\n",
      "0.93846154 LOSS:   167.82079 ADV_LOSS:   379.9181\n",
      "It has been 100.68388295173645 seconds since the loop started\n",
      "136\n",
      "global_step: 137\n",
      "It has been 79.79727005958557 seconds since the loop started\n",
      "137\n",
      "global_step: 138\n",
      "It has been 80.57029342651367 seconds since the loop started\n",
      "138\n",
      "global_step: 139\n",
      "It has been 79.26912927627563 seconds since the loop started\n",
      "139\n",
      "global_step: 140\n",
      "It has been 79.44723606109619 seconds since the loop started\n",
      "140\n",
      "global_step: 141\n",
      "0.94134617 LOSS:   164.1603 ADV_LOSS:   374.82895\n",
      "It has been 100.06091737747192 seconds since the loop started\n",
      "141\n",
      "global_step: 142\n",
      "It has been 81.5413658618927 seconds since the loop started\n",
      "142\n",
      "global_step: 143\n",
      "It has been 80.4881203174591 seconds since the loop started\n",
      "143\n",
      "global_step: 144\n",
      "It has been 79.07209920883179 seconds since the loop started\n",
      "144\n",
      "global_step: 145\n",
      "It has been 80.48927640914917 seconds since the loop started\n",
      "145\n",
      "global_step: 146\n",
      "0.9461538 LOSS:   156.77924 ADV_LOSS:   358.53195\n",
      "It has been 101.47262167930603 seconds since the loop started\n",
      "146\n",
      "global_step: 147\n",
      "It has been 82.08513593673706 seconds since the loop started\n",
      "147\n",
      "global_step: 148\n",
      "It has been 80.08442091941833 seconds since the loop started\n",
      "148\n",
      "global_step: 149\n",
      "It has been 80.68963384628296 seconds since the loop started\n",
      "149\n",
      "global_step: 150\n",
      "It has been 80.16042351722717 seconds since the loop started\n",
      "150\n",
      "global_step: 151\n",
      "0.9519231 LOSS:   155.65778 ADV_LOSS:   355.37442\n",
      "It has been 100.84831833839417 seconds since the loop started\n",
      "151\n",
      "global_step: 152\n",
      "It has been 80.85403418540955 seconds since the loop started\n",
      "152\n",
      "global_step: 153\n",
      "It has been 82.13350820541382 seconds since the loop started\n",
      "153\n",
      "global_step: 154\n",
      "It has been 82.74893426895142 seconds since the loop started\n",
      "154\n",
      "global_step: 155\n",
      "It has been 83.08026790618896 seconds since the loop started\n",
      "155\n",
      "global_step: 156\n",
      "0.9528846 LOSS:   154.03108 ADV_LOSS:   354.14618\n",
      "It has been 103.61707353591919 seconds since the loop started\n",
      "156\n",
      "global_step: 157\n",
      "It has been 81.30126142501831 seconds since the loop started\n",
      "157\n",
      "global_step: 158\n",
      "It has been 80.96638655662537 seconds since the loop started\n",
      "158\n",
      "global_step: 159\n",
      "It has been 80.35679078102112 seconds since the loop started\n",
      "159\n",
      "global_step: 160\n",
      "It has been 80.3879005908966 seconds since the loop started\n",
      "160\n",
      "global_step: 161\n",
      "0.95 LOSS:   150.33057 ADV_LOSS:   347.04324\n",
      "It has been 100.57298517227173 seconds since the loop started\n",
      "161\n",
      "global_step: 162\n",
      "It has been 79.17657208442688 seconds since the loop started\n",
      "162\n",
      "global_step: 163\n",
      "It has been 80.78232288360596 seconds since the loop started\n",
      "163\n",
      "global_step: 164\n",
      "It has been 81.33939695358276 seconds since the loop started\n",
      "164\n",
      "global_step: 165\n",
      "It has been 80.73857092857361 seconds since the loop started\n",
      "165\n",
      "global_step: 166\n",
      "0.95576924 LOSS:   145.99086 ADV_LOSS:   340.26633\n",
      "It has been 99.53331184387207 seconds since the loop started\n",
      "166\n",
      "global_step: 167\n",
      "It has been 81.13449954986572 seconds since the loop started\n",
      "167\n",
      "global_step: 168\n",
      "It has been 80.54708242416382 seconds since the loop started\n",
      "168\n",
      "global_step: 169\n",
      "It has been 79.4529800415039 seconds since the loop started\n",
      "169\n",
      "global_step: 170\n",
      "It has been 79.24303555488586 seconds since the loop started\n",
      "170\n",
      "global_step: 171\n",
      "0.94711536 LOSS:   147.65344 ADV_LOSS:   339.81024\n",
      "It has been 99.75854277610779 seconds since the loop started\n",
      "171\n",
      "global_step: 172\n",
      "It has been 81.16922235488892 seconds since the loop started\n",
      "172\n",
      "global_step: 173\n",
      "It has been 79.72549510002136 seconds since the loop started\n",
      "173\n",
      "global_step: 174\n",
      "It has been 80.7636833190918 seconds since the loop started\n",
      "174\n",
      "global_step: 175\n",
      "It has been 81.04736995697021 seconds since the loop started\n",
      "175\n",
      "global_step: 176\n",
      "0.9480769 LOSS:   149.00641 ADV_LOSS:   340.11545\n",
      "It has been 102.16912150382996 seconds since the loop started\n",
      "176\n",
      "global_step: 177\n",
      "It has been 82.65174269676208 seconds since the loop started\n",
      "177\n",
      "global_step: 178\n",
      "It has been 81.06978249549866 seconds since the loop started\n",
      "178\n",
      "global_step: 179\n",
      "It has been 82.76069331169128 seconds since the loop started\n",
      "179\n",
      "global_step: 180\n",
      "It has been 81.36129140853882 seconds since the loop started\n",
      "180\n",
      "global_step: 181\n",
      "0.9528846 LOSS:   144.27428 ADV_LOSS:   331.55264\n",
      "It has been 101.97281312942505 seconds since the loop started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n",
      "global_step: 182\n",
      "It has been 82.74758100509644 seconds since the loop started\n",
      "182\n",
      "global_step: 183\n",
      "It has been 83.01431179046631 seconds since the loop started\n",
      "183\n",
      "global_step: 184\n",
      "It has been 80.86888337135315 seconds since the loop started\n",
      "184\n",
      "global_step: 185\n",
      "It has been 81.14150214195251 seconds since the loop started\n",
      "185\n",
      "global_step: 186\n",
      "0.9548077 LOSS:   145.06113 ADV_LOSS:   333.35913\n",
      "It has been 101.65698838233948 seconds since the loop started\n",
      "186\n",
      "global_step: 187\n",
      "It has been 81.02862238883972 seconds since the loop started\n",
      "187\n",
      "global_step: 188\n",
      "It has been 81.26349425315857 seconds since the loop started\n",
      "188\n",
      "global_step: 189\n",
      "It has been 80.34475922584534 seconds since the loop started\n",
      "189\n",
      "global_step: 190\n",
      "It has been 80.46553087234497 seconds since the loop started\n",
      "190\n",
      "global_step: 191\n",
      "0.95 LOSS:   142.2545 ADV_LOSS:   330.7081\n",
      "It has been 103.1449704170227 seconds since the loop started\n",
      "191\n",
      "global_step: 192\n",
      "It has been 74.52345895767212 seconds since the loop started\n",
      "192\n",
      "global_step: 193\n",
      "It has been 75.32597231864929 seconds since the loop started\n",
      "193\n",
      "global_step: 194\n",
      "It has been 80.35063028335571 seconds since the loop started\n",
      "194\n",
      "global_step: 195\n",
      "It has been 80.97970175743103 seconds since the loop started\n",
      "195\n",
      "global_step: 196\n",
      "0.95576924 LOSS:   139.75014 ADV_LOSS:   325.49295\n",
      "It has been 95.24221992492676 seconds since the loop started\n",
      "196\n",
      "global_step: 197\n",
      "It has been 76.30013227462769 seconds since the loop started\n",
      "197\n",
      "global_step: 198\n",
      "It has been 81.83389329910278 seconds since the loop started\n",
      "198\n",
      "global_step: 199\n",
      "It has been 82.86198258399963 seconds since the loop started\n",
      "199\n",
      "global_step: 200\n",
      "It has been 83.19874286651611 seconds since the loop started\n",
      "200\n",
      "global_step: 201\n",
      "0.9519231 LOSS:   140.44492 ADV_LOSS:   323.6589\n",
      "It has been 103.03963732719421 seconds since the loop started\n",
      "201\n",
      "global_step: 202\n",
      "It has been 82.56235909461975 seconds since the loop started\n",
      "202\n",
      "global_step: 203\n",
      "It has been 82.22550535202026 seconds since the loop started\n",
      "203\n",
      "global_step: 204\n",
      "It has been 81.4680643081665 seconds since the loop started\n",
      "204\n",
      "global_step: 205\n",
      "It has been 80.9659595489502 seconds since the loop started\n",
      "205\n",
      "global_step: 206\n",
      "0.9528846 LOSS:   139.9439 ADV_LOSS:   322.77814\n",
      "It has been 105.06532502174377 seconds since the loop started\n",
      "206\n",
      "global_step: 207\n",
      "It has been 77.1521110534668 seconds since the loop started\n",
      "207\n",
      "global_step: 208\n",
      "It has been 80.53991460800171 seconds since the loop started\n",
      "208\n",
      "global_step: 209\n",
      "It has been 81.09161853790283 seconds since the loop started\n",
      "209\n",
      "global_step: 210\n",
      "It has been 82.2551634311676 seconds since the loop started\n",
      "210\n",
      "global_step: 211\n",
      "0.95384616 LOSS:   134.3138 ADV_LOSS:   313.15018\n",
      "It has been 102.76626110076904 seconds since the loop started\n",
      "211\n",
      "global_step: 212\n",
      "It has been 81.97582125663757 seconds since the loop started\n",
      "212\n",
      "global_step: 213\n",
      "It has been 81.65270709991455 seconds since the loop started\n",
      "213\n",
      "global_step: 214\n",
      "It has been 83.55222821235657 seconds since the loop started\n",
      "214\n",
      "global_step: 215\n",
      "It has been 82.38363456726074 seconds since the loop started\n",
      "215\n",
      "global_step: 216\n",
      "0.95384616 LOSS:   136.98764 ADV_LOSS:   318.406\n",
      "It has been 101.57834267616272 seconds since the loop started\n",
      "216\n",
      "global_step: 217\n",
      "It has been 81.6685528755188 seconds since the loop started\n",
      "217\n",
      "global_step: 218\n",
      "It has been 83.37140226364136 seconds since the loop started\n",
      "218\n",
      "global_step: 219\n",
      "It has been 81.78405356407166 seconds since the loop started\n",
      "219\n",
      "global_step: 220\n",
      "It has been 82.0668454170227 seconds since the loop started\n",
      "220\n",
      "global_step: 221\n",
      "0.9548077 LOSS:   133.00865 ADV_LOSS:   311.30008\n",
      "It has been 103.57168316841125 seconds since the loop started\n",
      "221\n",
      "global_step: 222\n",
      "It has been 82.57981777191162 seconds since the loop started\n",
      "222\n",
      "global_step: 223\n",
      "It has been 80.57364630699158 seconds since the loop started\n",
      "223\n",
      "global_step: 224\n",
      "It has been 81.67296981811523 seconds since the loop started\n",
      "224\n",
      "global_step: 225\n",
      "It has been 81.86127996444702 seconds since the loop started\n",
      "225\n",
      "global_step: 226\n",
      "0.95865387 LOSS:   132.62186 ADV_LOSS:   310.46643\n",
      "It has been 103.3685691356659 seconds since the loop started\n",
      "226\n",
      "global_step: 227\n",
      "It has been 80.54515624046326 seconds since the loop started\n",
      "227\n",
      "global_step: 228\n",
      "It has been 83.53713870048523 seconds since the loop started\n",
      "228\n",
      "global_step: 229\n",
      "It has been 81.08514547348022 seconds since the loop started\n",
      "229\n",
      "global_step: 230\n",
      "It has been 81.97707796096802 seconds since the loop started\n",
      "230\n",
      "global_step: 231\n",
      "0.95576924 LOSS:   132.48978 ADV_LOSS:   310.23563\n",
      "It has been 103.96023106575012 seconds since the loop started\n",
      "231\n",
      "global_step: 232\n",
      "It has been 82.95532083511353 seconds since the loop started\n",
      "232\n",
      "global_step: 233\n",
      "It has been 81.05017733573914 seconds since the loop started\n",
      "233\n",
      "global_step: 234\n",
      "It has been 82.26192045211792 seconds since the loop started\n",
      "234\n",
      "global_step: 235\n",
      "It has been 82.75506639480591 seconds since the loop started\n",
      "235\n",
      "global_step: 236\n",
      "0.9576923 LOSS:   133.15234 ADV_LOSS:   311.0232\n",
      "It has been 104.26470017433167 seconds since the loop started\n",
      "236\n",
      "global_step: 237\n",
      "It has been 81.0698561668396 seconds since the loop started\n",
      "237\n",
      "global_step: 238\n",
      "It has been 81.57089495658875 seconds since the loop started\n",
      "238\n",
      "global_step: 239\n",
      "It has been 79.06770205497742 seconds since the loop started\n",
      "239\n",
      "global_step: 240\n",
      "It has been 83.17104697227478 seconds since the loop started\n",
      "240\n",
      "global_step: 241\n",
      "0.95865387 LOSS:   131.26242 ADV_LOSS:   307.53888\n",
      "It has been 103.84453654289246 seconds since the loop started\n",
      "241\n",
      "global_step: 242\n",
      "It has been 78.81798434257507 seconds since the loop started\n",
      "242\n",
      "global_step: 243\n",
      "It has been 78.28091144561768 seconds since the loop started\n",
      "243\n",
      "global_step: 244\n",
      "It has been 80.07522082328796 seconds since the loop started\n",
      "244\n",
      "global_step: 245\n",
      "It has been 78.27606177330017 seconds since the loop started\n",
      "245\n",
      "global_step: 246\n",
      "0.9576923 LOSS:   129.30344 ADV_LOSS:   303.4621\n",
      "It has been 102.68232250213623 seconds since the loop started\n",
      "246\n",
      "global_step: 247\n",
      "It has been 81.85107564926147 seconds since the loop started\n",
      "247\n",
      "global_step: 248\n",
      "It has been 82.1956479549408 seconds since the loop started\n",
      "248\n",
      "global_step: 249\n",
      "It has been 81.56855368614197 seconds since the loop started\n",
      "249\n",
      "global_step: 250\n",
      "It has been 80.76768779754639 seconds since the loop started\n",
      "250\n",
      "global_step: 251\n",
      "0.9596154 LOSS:   131.17896 ADV_LOSS:   306.58096\n",
      "It has been 104.28901648521423 seconds since the loop started\n",
      "251\n",
      "global_step: 252\n",
      "It has been 82.01459312438965 seconds since the loop started\n",
      "252\n",
      "global_step: 253\n",
      "It has been 83.12670469284058 seconds since the loop started\n",
      "253\n",
      "global_step: 254\n",
      "It has been 83.28123426437378 seconds since the loop started\n",
      "254\n",
      "global_step: 255\n",
      "It has been 83.1578323841095 seconds since the loop started\n",
      "255\n",
      "global_step: 256\n",
      "0.96153843 LOSS:   129.95863 ADV_LOSS:   304.2375\n",
      "It has been 103.45180988311768 seconds since the loop started\n",
      "256\n",
      "global_step: 257\n",
      "It has been 80.86933970451355 seconds since the loop started\n",
      "257\n",
      "global_step: 258\n",
      "It has been 79.48181223869324 seconds since the loop started\n",
      "258\n",
      "global_step: 259\n",
      "It has been 82.25994729995728 seconds since the loop started\n",
      "259\n",
      "global_step: 260\n",
      "It has been 81.17111206054688 seconds since the loop started\n",
      "260\n",
      "global_step: 261\n",
      "0.96057695 LOSS:   126.76283 ADV_LOSS:   298.80545\n",
      "It has been 102.17012166976929 seconds since the loop started\n",
      "261\n",
      "global_step: 262\n",
      "It has been 82.69852328300476 seconds since the loop started\n",
      "262\n",
      "global_step: 263\n",
      "It has been 82.20558762550354 seconds since the loop started\n",
      "263\n",
      "global_step: 264\n",
      "It has been 81.08222842216492 seconds since the loop started\n",
      "264\n",
      "global_step: 265\n",
      "It has been 82.08079171180725 seconds since the loop started\n",
      "265\n",
      "global_step: 266\n",
      "0.9567308 LOSS:   128.75488 ADV_LOSS:   302.30707\n",
      "It has been 104.13854956626892 seconds since the loop started\n",
      "266\n",
      "global_step: 267\n",
      "It has been 82.25575375556946 seconds since the loop started\n",
      "267\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-7dcd5a365dc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_in_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_sizee\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'global_step: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/.packages/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/.packages/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/.packages/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/.packages/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/.packages/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/.packages/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#####Training Begins Here######\n",
    "#####Training Begins Here######\n",
    "#####Training Begins Here######\n",
    "#####Training Begins Here######\n",
    "#####Training Begins Here######\n",
    "\n",
    "save_flag = False\n",
    "program_starts = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saved_path = saver.restore(sess,stk1[:6]+'tmt.ckpt')\n",
    "    sess.run(switch_to_train_mode_op)\n",
    "    for epoch in np.arange(500):\n",
    "        print(epoch)\n",
    "        # for i in np.arange(int(len(trainX)/batch_size)):\n",
    "        loop_starts = time.time()\n",
    "        batch_x = trainX\n",
    "        batch_y = trainY\n",
    "        sess.run(train_op, feed_dict={input_data: batch_x, targets: batch_y,keep_prob:0.95,keep_in_prob: 0.5,batch_sizee:np.int64(batch_x.shape[0])})\n",
    "        print('global_step: %s' % tf.compat.v1.train.global_step(sess,global_step))\n",
    "\n",
    "        if epoch % 5 == 0 or epoch==284 or loss_list[-1] < 50:\n",
    "            acc, adv_loss_2,loss_2= sess.run([accuracy, adv_loss,loss], feed_dict={input_data: batch_x, targets: batch_y,keep_prob: 1.0,keep_in_prob: 1.0,batch_sizee:np.int64(batch_x.shape[0])})\n",
    "            print(acc,\"LOSS:  \",loss_2, \"ADV_LOSS:  \",adv_loss_2)\n",
    "            saved_path = saver.save(sess,stk1[:6]+'tmt.ckpt')\n",
    "\n",
    "            if (loss_2 < 40 and loss_list[-1] - loss_2 < 4 and acc >= 0.99) or epoch == 184:\n",
    "                saved_path = saver.save(sess,stk1[:6]+'tmt.ckpt')\n",
    "                \n",
    "                while not save_flag:\n",
    "            \n",
    "                    try:\n",
    "\n",
    "                        sess.run(switch_to_test_mode_op)\n",
    "\n",
    "                        acc, adv_loss_2,loss_2= sess.run([accuracy, adv_loss,loss], feed_dict={input_data: batch_x, targets: batch_y,keep_prob: 1.0,keep_in_prob: 1.0,batch_sizee:np.int64(batch_x.shape[0])})\n",
    "                        print(acc,\"LOSS:  \",loss_2, \"ADV_LOSS:  \",adv_loss_2)\n",
    "                        saved_path = saver.save(sess,stk1[:7]+'tmt.ckpt')\n",
    "                        #sess.run(switch_to_train_mode_op)\n",
    "                        save_flag = True\n",
    "                        print(\"sucessfully saved\")\n",
    "                        \n",
    "                    except:\n",
    "                        print('failed to save')\n",
    "                        pass\n",
    "\n",
    "\n",
    "                break\n",
    "                \n",
    "            loss_list.append(loss_2)\n",
    "            acc_list.append(acc)\n",
    "        pred = sess.run(accuracy, feed_dict={input_data: test_X, targets: test_Y,keep_prob: 1.0,keep_in_prob: 1.0,batch_sizee:np.int64(test_X.shape[0])})\n",
    "        acc_pred_list.append(pred)\n",
    "        end_loop = time.time()\n",
    "        \n",
    "        print(\"It has been {0} seconds since the loop started\".format(end_loop - loop_starts))\n",
    "        saved_path = saver.save(sess,stk1[:6]+'tmt.ckpt')\n",
    "        \n",
    "now = time.time()\n",
    "print(\"It has been {0} seconds since the ENTIRE THING started\".format(now - program_starts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "  def __init__(self, model_path,period):\n",
    "    # Note, if you don't want to leak this, you'll want to turn Model into\n",
    "    # a context manager. In practice, you probably don't have to worry\n",
    "    # about it.\n",
    "    self.model_path = model_path\n",
    "    self.session = tf.Session()\n",
    "    self.period = period\n",
    "    #self.session.run(tf.global_variables_initializer())\n",
    "   \n",
    "    saver.restore(self.session,self.model_path)\n",
    "\n",
    "    #self.softmax_tensor = self.session.graph.get_tensor_by_name('final_ops/softmax:0')\n",
    "    self.clas = tf.nn.softmax(final_output)\n",
    "    #self.clas = final_output\n",
    "    #tg = sess.run([clas], feed_dict={input_data:trainX})\n",
    "  def predict(self, images):\n",
    "    predictions = self.session.run(self.clas, {input_data: images,keep_prob: 1.0, keep_in_prob: 1.0,batch_sizee:np.int64(self.period)})\n",
    "    # TODO: convert to human-friendly labels\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rice/notebook/.packages/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡åž‹ï¼š \t 600881tmt.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = Model(stk1[:6]+'tmt.ckpt',137)\n",
    "print('æ¨¡åž‹ï¼š \\t',stk1[:6]+'tmt.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
